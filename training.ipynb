{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from rlpyt.samplers.collections import TrajInfo\n",
    "from rlpyt.runners.minibatch_rl import MinibatchRlEval, MinibatchRl\n",
    "from rlpyt.samplers.serial.sampler import SerialSampler\n",
    "from rlpyt.utils.logging.context import logger_context\n",
    "\n",
    "from models.agent import DMCDreamerAgent\n",
    "from algorithm import Dreamer\n",
    "from envs.dmc import DeepMindControl\n",
    "from envs.time_limit import TimeLimit\n",
    "from envs.action_repeat import ActionRepeat\n",
    "from envs.normalize_actions import NormalizeActions\n",
    "from envs.wrapper import make_wapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(\n",
    "    log_dir,\n",
    "    game=\"cartpole_balance\",\n",
    "    run_ID=0,\n",
    "    cuda_idx=None,\n",
    "    eval=False,\n",
    "    save_model=\"last\",\n",
    "    load_model_path=None,\n",
    "):\n",
    "    params = torch.load(load_model_path) if load_model_path else {}\n",
    "    agent_state_dict = params.get(\"agent_state_dict\")\n",
    "    optimizer_state_dict = params.get(\"optimizer_state_dict\")\n",
    "\n",
    "    action_repeat = 2\n",
    "    factory_method = make_wapper(\n",
    "        DeepMindControl,\n",
    "        [ActionRepeat, NormalizeActions, TimeLimit],\n",
    "        [dict(amount=action_repeat), dict(), dict(duration=1000 / action_repeat)],\n",
    "    )\n",
    "    sampler = SerialSampler(\n",
    "        EnvCls=factory_method,\n",
    "        TrajInfoCls=TrajInfo,\n",
    "        env_kwargs=dict(name=game),\n",
    "        eval_env_kwargs=dict(name=game),\n",
    "        batch_T=1,\n",
    "        batch_B=1,\n",
    "        max_decorrelation_steps=0,\n",
    "        eval_n_envs=10,\n",
    "        eval_max_steps=int(10e3),\n",
    "        eval_max_trajectories=5,\n",
    "    )\n",
    "    algo = Dreamer(initial_optim_state_dict=optimizer_state_dict)  # Run with defaults.\n",
    "    agent = DMCDreamerAgent(\n",
    "        train_noise=0.3,\n",
    "        eval_noise=0,\n",
    "        expl_type=\"additive_gaussian\",\n",
    "        expl_min=None,\n",
    "        expl_decay=None,\n",
    "        initial_model_state_dict=agent_state_dict,\n",
    "    )\n",
    "    runner_cls = MinibatchRlEval if eval else MinibatchRl\n",
    "    runner = runner_cls(\n",
    "        algo=algo,\n",
    "        agent=agent,\n",
    "        sampler=sampler,\n",
    "        n_steps=5e6,\n",
    "        log_interval_steps=1e3,\n",
    "        affinity=dict(cuda_idx=cuda_idx),\n",
    "    )\n",
    "    config = dict(game=game)\n",
    "    name = \"dreamer_\" + game\n",
    "    with logger_context(\n",
    "        log_dir,\n",
    "        run_ID,\n",
    "        name,\n",
    "        config,\n",
    "        snapshot_mode=save_model,\n",
    "        override_prefix=True,\n",
    "        use_summary_writer=True,\n",
    "    ):\n",
    "        runner.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\"--game\", help=\"DMC game\", default=\"cartpole_balance\")\n",
    "    parser.add_argument(\n",
    "        \"--run-ID\", help=\"run identifier (logging)\", type=int, default=0\n",
    "    )\n",
    "    parser.add_argument(\"--cuda-idx\", help=\"gpu to use \", type=int, default=None)\n",
    "    parser.add_argument(\"--eval\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--save-model\",\n",
    "        help=\"save model\",\n",
    "        type=str,\n",
    "        default=\"last\",\n",
    "        choices=[\"all\", \"none\", \"gap\", \"last\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load-model-path\", help=\"load model from path\", type=str\n",
    "    )  # path to params.pkl\n",
    "    default_log_dir = os.path.join(\n",
    "        os.path.dirname(__file__),\n",
    "        \"data\",\n",
    "        \"local\",\n",
    "        datetime.datetime.now().strftime(\"%Y%m%d\"),\n",
    "    )\n",
    "    parser.add_argument(\"--log-dir\", type=str, default=default_log_dir)\n",
    "    args = parser.parse_args()\n",
    "    log_dir = os.path.abspath(args.log_dir)\n",
    "    i = args.run_ID\n",
    "    while os.path.exists(os.path.join(log_dir, \"run_\" + str(i))):\n",
    "        print(f\"run {i} already exists. \")\n",
    "        i += 1\n",
    "    print(f\"Using run id = {i}\")\n",
    "    args.run_ID = i\n",
    "    build_and_train(\n",
    "        log_dir,\n",
    "        game=args.game,\n",
    "        run_ID=args.run_ID,\n",
    "        cuda_idx=args.cuda_idx,\n",
    "        eval=args.eval,\n",
    "        save_model=args.save_model,\n",
    "        load_model_path=args.load_model_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.abspath('/home/eddy/Projects/RL_project/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AgentModel.__init__() missing 1 required positional argument: 'action_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuild_and_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHumanoidStandup-v4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_ID\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcuda_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m, in \u001b[0;36mbuild_and_train\u001b[0;34m(log_dir, game, run_ID, cuda_idx, eval, save_model, load_model_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m sampler \u001b[38;5;241m=\u001b[39m SerialSampler(\n\u001b[1;32m     21\u001b[0m     EnvCls\u001b[38;5;241m=\u001b[39mfactory_method,\n\u001b[1;32m     22\u001b[0m     TrajInfoCls\u001b[38;5;241m=\u001b[39mTrajInfo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     eval_max_trajectories\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m algo \u001b[38;5;241m=\u001b[39m Dreamer(initial_optim_state_dict\u001b[38;5;241m=\u001b[39moptimizer_state_dict)  \u001b[38;5;66;03m# Run with defaults.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDMCDreamerAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpl_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditive_gaussian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpl_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpl_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_model_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m runner_cls \u001b[38;5;241m=\u001b[39m MinibatchRlEval \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MinibatchRl\n\u001b[1;32m     42\u001b[0m runner \u001b[38;5;241m=\u001b[39m runner_cls(\n\u001b[1;32m     43\u001b[0m     algo\u001b[38;5;241m=\u001b[39malgo,\n\u001b[1;32m     44\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     affinity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(cuda_idx\u001b[38;5;241m=\u001b[39mcuda_idx),\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/RL_project/models/agent.py:175\u001b[0m, in \u001b[0;36mDMCDreamerAgent.__init__\u001b[0;34m(self, ModelCls, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ModelCls\u001b[38;5;241m=\u001b[39mAtariDreamerModel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModelCls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelCls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: AgentModel.__init__() missing 1 required positional argument: 'action_shape'"
     ]
    }
   ],
   "source": [
    "build_and_train(\n",
    "        log_dir,\n",
    "        game=\"HumanoidStandup-v4\",\n",
    "        run_ID=0,\n",
    "        cuda_idx=0,\n",
    "        eval=False,\n",
    "        save_model=\"last\",\n",
    "        load_model_path=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
